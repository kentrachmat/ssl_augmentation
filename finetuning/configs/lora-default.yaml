lora:
r: 16
alpha: 32
dropout: 0.05
target_modules:
# For Llama/Qwen attention + mlp modules; transformers will skip if missing
- q_proj
- k_proj
- v_proj
- o_proj
- gate_proj
- up_proj
- down_proj
bias: none
task_type: CAUSAL_LM
quantization:
qlora: true # set false for plain LoRA (fp16/bf16)
load_in_4bit: true
bnb_4bit_compute_dtype: bfloat16
bnb_4bit_quant_type: nf4
bnb_4bit_use_double_quant: true
training:
num_train_epochs: 2
per_device_train_batch_size: 2
per_device_eval_batch_size: 2
gradient_accumulation_steps: 8
learning_rate: 2.0e-4
lr_scheduler_type: cosine
weight_decay: 0.0
warmup_ratio: 0.03
max_grad_norm: 1.0
logging_steps: 10
save_steps: 500
eval_steps: 500
gradient_checkpointing: true
bf16: true
deepspeed: deepspeed/ds_zero2.json # set null to disable
max_seq_length: 4096 # truncate packed sequence length
packing: false # set true if you pack multiple examples per sequence
seed: 42