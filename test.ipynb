{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf01979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, re, string\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "results_dir = \"results\"\n",
    "files = {\n",
    "    \"Qwen2.5-7B\": \"predictions_techqa_llama.jsonl\",\n",
    "    \"LLaMA-3.1-8B\": \"predictions_techqa_qwen.jsonl\"\n",
    "}\n",
    "csv_out = \"squadv2_metrics_em_f1.csv\"\n",
    "\n",
    "def normalize_answer(s: str) -> str:\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = s.lower()\n",
    "    exclude = set(string.punctuation)\n",
    "    s = \"\".join(ch for ch in s if ch not in exclude)\n",
    "    s = re.sub(r\"\\b(a|an|the)\\b\", \" \", s)\n",
    "    s = \" \".join(s.split())\n",
    "    return s\n",
    "\n",
    "def f1_score(prediction: str, ground_truth: str) -> float:\n",
    "    pred = normalize_answer(prediction).split()\n",
    "    gold = normalize_answer(ground_truth).split()\n",
    "    if len(gold) == 0 and len(pred) == 0:\n",
    "        return 1.0\n",
    "    if len(gold) == 0 or len(pred) == 0:\n",
    "        return 0.0\n",
    "    common = defaultdict(int)\n",
    "    for t in gold:\n",
    "        common[t] += 1\n",
    "    num_same = 0\n",
    "    for t in pred:\n",
    "        if common[t] > 0:\n",
    "            num_same += 1\n",
    "            common[t] -= 1\n",
    "    if num_same == 0:\n",
    "        return 0.0\n",
    "    precision = num_same / len(pred)\n",
    "    recall = num_same / len(gold)\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "def exact_match_score(prediction: str, ground_truth: str) -> int:\n",
    "    return int(normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "def read_jsonl(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                yield json.loads(line)\n",
    "\n",
    "def evaluate_file(jsonl_path: str):\n",
    "    total = 0\n",
    "    em_sum = 0\n",
    "    f1_sum = 0.0\n",
    "    for ex in read_jsonl(jsonl_path):\n",
    "        pred = ex.get(\"predicted_answer\", \"\")\n",
    "        gold = ex.get(\"gold_answer\", \"\")\n",
    "        em_sum += exact_match_score(pred, gold)\n",
    "        f1_sum += f1_score(pred, gold)\n",
    "        total += 1\n",
    "    if total == 0:\n",
    "        return {\"exact\": 0.0, \"f1\": 0.0, \"total\": 0}\n",
    "    return {\n",
    "        \"exact\": 100.0 * em_sum / total,\n",
    "        \"f1\": 100.0 * (f1_sum / total),\n",
    "        \"total\": total\n",
    "    }\n",
    "\n",
    "rows = []\n",
    "for model, fname in files.items():\n",
    "    path = os.path.join(results_dir, fname)\n",
    "    if os.path.exists(path):\n",
    "        res = evaluate_file(path)\n",
    "        rows.append({\"model\": model, \"EM\": res[\"exact\"], \"F1\": res[\"f1\"], \"total\": res[\"total\"]})\n",
    "    else:\n",
    "        rows.append({\"model\": model, \"EM\": None, \"F1\": None, \"total\": 0})\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "plot_df = df.dropna(subset=[\"EM\", \"F1\"]).copy()\n",
    "\n",
    "# Sort by F1 desc for nicer comparison\n",
    "if len(plot_df) > 0:\n",
    "    plot_df = plot_df.sort_values(by=\"F1\", ascending=False)\n",
    "\n",
    "df.to_csv(csv_out, index=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "if len(plot_df) > 0:\n",
    "    models = plot_df[\"model\"].tolist()\n",
    "    x = range(len(models))\n",
    "    bar_width = 0.35\n",
    "\n",
    "    em_scores = plot_df[\"EM\"].tolist()\n",
    "    f1_scores = plot_df[\"F1\"].tolist()\n",
    "\n",
    "    em_bars = plt.bar([i - bar_width/2 for i in x], em_scores, width=bar_width, label=\"EM\")\n",
    "    f1_bars = plt.bar([i + bar_width/2 for i in x], f1_scores, width=bar_width, label=\"F1\")\n",
    "\n",
    "    plt.xticks(list(x), models, rotation=15)\n",
    "    plt.ylabel(\"Score (%)\")\n",
    "    plt.ylim(0, 100)\n",
    "    plt.title(\"SQuAD v2.0 Manual Evaluation: EM vs F1\")\n",
    "\n",
    "    # Add value labels on bars\n",
    "    def autolabel(bars):\n",
    "        for b in bars:\n",
    "            h = b.get_height()\n",
    "            plt.text(b.get_x() + b.get_width()/2, h + 1, f\"{h:.1f}\", ha='center', va='bottom', fontsize=9)\n",
    "    autolabel(em_bars)\n",
    "    autolabel(f1_bars)\n",
    "\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
    "\n",
    "    plt.legend()\n",
    "else:\n",
    "    plt.text(0.5, 0.5, \"No evaluable models found.\\n(Check file paths.)\", ha=\"center\", va=\"center\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
